{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1: Platform Classification using Supervised Learning Methods - Building Training and Test datasets - paperbot data (part 3 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "* https://www.quora.com/What-is-the-best-way-to-create-a-training-set-for-machine-learning/answer/Clem-Wang-1\n",
    "* http://www.kdnuggets.com/2017/06/acquiring-quality-labeled-training-data.html\n",
    "* http://www.kdnuggets.com/datasets/index.html\n",
    "* https://developer.similarweb.com/website_categorization_API\n",
    "* http://solutionfile.trendmicro.com/solutionfile/Consumer/new-web-classification.html\n",
    "* http://data.webarchive.org.uk/opendata/ukwa.ds.1/classification/\n",
    "* https://www.whoisxmlapi.com/blog/moving-beyond-whois-introducing-website-classification-database/\n",
    "* https://zvelo.com/zvelo-products/zvelos-data-platform-zvelodp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "from IPython.display import display, Math, Latex #also '%%latex' magic command\n",
    "import collections, itertools, operator, re, copy, datetime\n",
    "import urllib, urllib.request, urllib.parse, dns, ipwhois\n",
    "import pickle, json, csv, zipfile\n",
    "import math, random, numpy, scipy, pandas\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import bs4\n",
    "import nltk, sklearn\n",
    "\n",
    "#actualcwd = os.getcwd()\n",
    "#os.chdir(actualcwd)\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "#a simple but not adequate hack to solve:\n",
    "#-- that this notebook will include any passed module that exists on top of its folder\n",
    "#-- that my installation of the the nltk will find the correct path to the nlkt_data folder\n",
    "#\n",
    "#once this line is run, IT SHOULDN'T RUN AGAIN!, otherwise the cwd will change into something different; \n",
    "#it can restored though in different ways, one by using `actualcwd` variable or in some cases shuting down this notebook\n",
    "#\n",
    "#\n",
    "os.chdir('../..')\n",
    "#print(os.getcwd())\n",
    "sys.path.append(os.getcwd())\n",
    "import config.config as config\n",
    "\n",
    "#print(config.anacondadir)\n",
    "\n",
    "if os.path.exists(config.anacondadir):\n",
    "    print('ok')\n",
    "    nltk.data.path.append(config.anacondadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-bfc7f4b2e6e8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-bfc7f4b2e6e8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    STOP HERE!!\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "STOP HERE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataloading(form='', status=''):\n",
    "    #OBSERVATION!: name of file has been changed to reflect last updated file \n",
    "    if pathlib.Path(os.getcwd()+'/data/'+form+'platformsphase1_a'+status+'.csv').is_file():\n",
    "        pd = pandas.read_csv(open(os.getcwd()+'/data/'+form+'platformsphase1_a'+status+'.csv', 'r'), sep=';', quotechar=\"'\")\n",
    "    else:\n",
    "        print('Not path found')\n",
    "    print(pd.head(5))\n",
    "    if 'missing' in pd.columns:\n",
    "        print(pd.groupby(['missing'])['missing'].count())\n",
    "        print(pd.loc[pd.missing > 0,:])\n",
    "    print(pd.shape)\n",
    "    return pd\n",
    "\n",
    "def datawikiupdating(pdform):\n",
    "    import scr.wikipedia_extract\n",
    "    import imp\n",
    "    imp.reload(scr.wikipedia_extract)\n",
    "    pdform['wiki'] = ''\n",
    "    scr.wikipedia_extract.getting_wikipedia(pdform)\n",
    "    print(pdform['wiki'][:50])\n",
    "    \n",
    "def datasaving(pd, form = 'annotated', status=''):\n",
    "    if not pathlib.Path(os.getcwd()+'/data/'+form+'platformsphase1_a'+status+'.csv').is_file():\n",
    "        pd.to_csv(os.getcwd()+'/data/'+form+'platformsphase1_a'+status+'.csv', sep=';', quotechar=\"'\")\n",
    "        print('The file : ', form+'platformsphase1_a'+status, 'has been successfully saved. Please check.')\n",
    "    else:\n",
    "        accept = input('You are to overwrite the following file : {0}.\\nIs that correct?\\n'.format(os.getcwd()+'/data/'+form+'platformsphase1_a'+status+'.csv'))\n",
    "        if accept == 'yes':\n",
    "            pd.to_csv(os.getcwd()+'/data/'+form+'platformsphase1_a'+status+'.csv', sep=';', quotechar=\"'\")\n",
    "            print('The file has been overwritten. Please check.')\n",
    "        else:\n",
    "            print('Overwriting aborted.' )\n",
    "\n",
    "def datapreparation(pdform):\n",
    "    import imp\n",
    "    import scr.datapreparation_ML\n",
    "    imp.reload(scr.datapreparation_ML)\n",
    "\n",
    "    def jscheck(x):\n",
    "        plat = x\n",
    "        if len(plat.split('.')) > 1:\n",
    "            if len(plat.split('.')[-2]) > 5:\n",
    "                if plat.split('.')[-2][-2:] == 'js':\n",
    "                    return 1\n",
    "            elif plat.split('.')[0] == 'api':\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def docscheck(x):\n",
    "        plat = x\n",
    "        if len(plat.split('.')) > 1:\n",
    "            if len(plat.split('.')[0]) > 2:\n",
    "                if plat.split('.')[0] in ['docs', 'dev', 'developers', 'developer', 'plugins']:\n",
    "                    return 1\n",
    "        return 0\n",
    "    \n",
    "    def blogscheck(x):\n",
    "        plat = x\n",
    "        if len(plat.split('.')) > 1:\n",
    "            if len(plat.split('.')[0]) > 2:\n",
    "                if plat.split('.')[0] in ['blog', 'blogs', 'articule', 'articules', 'weblogs', 'weblog']:\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    def commcheck(x):\n",
    "        plat = x\n",
    "        if len(plat.split('.')) > 1:\n",
    "            if len(plat.split('.')[0]) > 2:\n",
    "                if plat.split('.')[0] in ['forum', 'forums', 'community', 'channel']:\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    pdform['alltext'] = ''\n",
    "    scr.datapreparation_ML.datapreparation(pdform)\n",
    "\n",
    "    pdform['jscheck'] = None\n",
    "    pdform['docscheck'] = None\n",
    "    pdform['blogscheck'] = None\n",
    "    pdform['commcheck'] = None\n",
    "    \n",
    "    pdform['jscheck'] = pdform['platform'].apply(jscheck)\n",
    "    pdform['docscheck'] = pdform['platform'].apply(docscheck)\n",
    "    pdform['blogscheck'] = pdform['platform'].apply(blogscheck)\n",
    "    pdform['commcheck'] = pdform['platform'].apply(commcheck)    \n",
    "    \n",
    "    \n",
    "#ojo to work with local variables:\n",
    "#https://stackoverflow.com/a/932835/3675901\n",
    "#locals()[\"i\"]\n",
    "    \n",
    "def dataloadandprep(loading=(True,True), status='', increasesizetrain=False, wiki=(False,False), prep=(False,False)):\n",
    "    '''\n",
    "    --- loading\n",
    "    --- form\n",
    "    --- status: current id of the cycle \n",
    "    --- wiki\n",
    "    --- prep\n",
    "    --- increasesizetrain\n",
    " \n",
    "    pd_annotated = pdfiles[0]\n",
    "    pd_notated = pdfiles[1]\n",
    " \n",
    "    '''\n",
    "    pdfiles = []\n",
    "    for i, v in enumerate(loading):\n",
    "        if v:\n",
    "            if i == 0:\n",
    "                form = 'annotated'\n",
    "            else:\n",
    "                form = 'notated'\n",
    "            pdfiles.append(dataloading(form,status))\n",
    "    \n",
    "\n",
    "    \n",
    "    if increasesizetrain:\n",
    "        unform = 'unlabelled'\n",
    "        if pathlib.Path(os.getcwd()+'/data/'+unform+'platformsphase1_a'+status+'.csv').is_file():\n",
    "            pd_predaggregate_trans = pandas.read_csv(open(os.getcwd()+'/data/'+unform+'platformsphase1_a'+status+'.csv', 'r'), sep=';', quotechar=\"'\")\n",
    "            print('Could open ', os.getcwd()+'/data/'+unform+'platformsphase1_a'+status+'.csv', 'successfully')\n",
    "        else:\n",
    "            print('Could not open ', os.getcwd()+'/data/'+unform+'platformsphase1_a'+status+'.csv')\n",
    "            return\n",
    "        print(pd_predaggregate_trans.columns, pd_predaggregate_trans.shape)\n",
    "        pd_predaggregate_trans = pd_predaggregate_trans[['platform','category']]\n",
    "        pd_annotated_trans = pandas.merge(pdfiles[1].drop(['category'],axis=1), pd_predaggregate_trans, on='platform')\n",
    "        print(pd_annotated_trans.shape, pd_annotated_trans.columns, pd_annotated_trans.category)\n",
    "        pd_annotated_x = pdfiles[0].loc[:,~pdfiles[0].columns.duplicated()]\n",
    "        pd_annotated_trans = pd_annotated_trans.loc[:,~pd_annotated_trans.columns.duplicated()]\n",
    "        pd_annotated_x = pandas.concat([pd_annotated_x,pd_annotated_trans], join='inner', ignore_index=True)\n",
    "        print(pd_annotated_x.shape, pd_annotated_x.columns)\n",
    "        accept = input('Would you like to write to file to {0}?\\n'.format(os.getcwd()+'/data/annotatedplatformsphase1_a'+str(int(status)+1)+'.csv'))\n",
    "        if accept.lower() == 'yes':\n",
    "            pdfiles[0] = pd_annotated_x\n",
    "            datasaving(pdfiles[0], form = 'annotated', status=str(int(status)+1))\n",
    "            return\n",
    "            \n",
    "        else:\n",
    "            sys.exit()\n",
    "    \n",
    "    for i, v in enumerate(wiki):\n",
    "        if v:\n",
    "            datawikiupdating(pdfiles[i])\n",
    "    \n",
    "    for i, v in enumerate(prep):\n",
    "        #for col in pdform.columns:\n",
    "        #    print(col,' has the following null values: ', pdform[[col]].isnull().sum())\n",
    "        if v:\n",
    "            datapreparation(pdfiles[i])\n",
    "            if i == 0:\n",
    "                form = 'annotated'\n",
    "            else:\n",
    "                form = 'notated'\n",
    "            datasaving(pdfiles[i], form, status)\n",
    "\n",
    "    return pdfiles\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA UPDATING - Important..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code commented below was used to:\n",
    "* **Add wikipedia info to the created tables**: for this I used the `scr.wikipedia_extract` module made for this purpose\n",
    "* **Revise some column naming and use the names for categories** to be used from now on: until now (29-July-17), I was using as categories a regex form I used for the original classification rules; from now I will be using the defined categories found in the `docs` folder of this work\n",
    "\n",
    "The changes below were done by creating updated files, re-opening that file and update it until the right file was created. Once the information was updated with some information, the respective code was commented because it was not of usage any more.\n",
    "\n",
    "The code below is kept commented but is shown so other users of this file can have a reference. Also the different intermediate files are kept in the `data` folder but only the required files are called for further analysis. In a more advanced phase a simpler file will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_errortemp = pd_notated.loc[pd_notated['platform'] == 'view-source:http:',]\n",
    "#scr.wikipedia_extract.getting_wikipedia(pd_errortemp)\n",
    "#print(pd_errortemp)\n",
    "#print(pd_notated.iloc[110,])\n",
    "#url = 'https://en.wikipedia.org/w/api.php?action=query&list=search&format=json&srsearch=JavaScript Objects in Detail'\n",
    "#req = urllib.request.Request(url)\n",
    "#try:\n",
    "#    resp = urllib.request.urlopen(req)\n",
    "#except urllib.error.HTTPError as e:\n",
    "#    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd_annotated = pd_annotated.rename(columns={'category':'category_regex'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_annotated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#categories = pandas.read_csv(open(os.getcwd()+'/docs/category_operationalization.csv', 'r'), sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#categories = categories[['category','category_regex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_annotated = pandas.merge(pd_annotated, categories, on=['category_regex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd_annotated.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd_annotated.to_csv(os.getcwd()+'/data/annotatedplatformsphase1_a1.csv', sep=';', quotechar=\"'\")\n",
    "#pd_notated.to_csv(os.getcwd()+'/data/notatedplatformsphase1_a2.csv', sep=';', quotechar=\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-d5af98942240>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-d5af98942240>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    STOP HERE!\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "STOP HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1                   platform  \\\n",
      "0           0             0  v4-alpha.getbootstrap.com   \n",
      "1           1             1                jscroll.com   \n",
      "2           2             2             mongoosejs.com   \n",
      "3           3             3         www.highcharts.com   \n",
      "4           4             4         www.crossorigin.me   \n",
      "\n",
      "                                               title  \\\n",
      "0  \\n  \\n    bootstrap · the most popular html, c...   \n",
      "1  jscroll - a jquery plugin for infinite scrolli...   \n",
      "2                               mongoose odm v4.10.5   \n",
      "3  Interactive JavaScript charts for your webpage...   \n",
      "4                                     crossorigin.me   \n",
      "\n",
      "                                         description  \\\n",
      "0  the most popular html, css, and js framework i...   \n",
      "1                                                      \n",
      "2                                                      \n",
      "3  Highcharts - Interactive JavaScript charts for...   \n",
      "4                                                      \n",
      "\n",
      "                                            keywords  \\\n",
      "0                                                      \n",
      "1                                                      \n",
      "2                                                      \n",
      "3  highcharts, charts, javascript charts, ajax ch...   \n",
      "4                                                      \n",
      "\n",
      "                                               htext  \\\n",
      "0  Bootstrap is the most popular HTML, CSS, and J...   \n",
      "1  jScroll is a jQuery plugin for infinite scroll...   \n",
      "2  Elegant MongoDB object modeling for Node.js Mo...   \n",
      "3                           View demo Get a license    \n",
      "4  \\n                  Welcome to crossorigin.me,...   \n",
      "\n",
      "                                              params wiki  \\\n",
      "0  ,/components,/components/breadcrumb/,/migratio...  NaN   \n",
      "1                 /,https://github.com/cubiq/iscroll  NaN   \n",
      "2  /docs/promises.html,/docs/populate.html,/docs/...  NaN   \n",
      "3  /samples/data/jsonp.php,/blog/192-use-highchar...  NaN   \n",
      "4  /https://en.wikipedia.org/w/api.php,/,/https:/...  NaN   \n",
      "\n",
      "                                             alltext  jscheck  docscheck  \\\n",
      "0    components clearfix world carousel buttons h...        0          0   \n",
      "1    plugin loaded github new com iscroll cubiq e...        0          0   \n",
      "2    validation mongoose models node queries api ...        1          0   \n",
      "3    charts samples jsonp navigator view highchar...        0          0   \n",
      "4    crossorigin org wikipedia api w free https e...        0          0   \n",
      "\n",
      "   blogscheck  commcheck category  \n",
      "0           0          0  PACKAGE  \n",
      "1           0          0  PACKAGE  \n",
      "2           0          0  PACKAGE  \n",
      "3           0          0  PACKAGE  \n",
      "4           0          0  PACKAGE  \n",
      "(1096, 15)\n",
      "   Unnamed: 0  Unnamed: 0.1             platform  \\\n",
      "0           0             0    andrewhfarmer.com   \n",
      "1           1             1      webpack.academy   \n",
      "2           2             2          git-scm.com   \n",
      "3           3             3       devopscube.com   \n",
      "4           4             4  firebase.google.com   \n",
      "\n",
      "                                               title  \\\n",
      "0                                                NaN   \n",
      "1                    Home | webpack learning academy   \n",
      "2                                                Git   \n",
      "3  DevopsCube - DevOps trends, News, tutorials, c...   \n",
      "4                                           Firebase   \n",
      "\n",
      "                                         description            keywords  \\\n",
      "0                                                NaN                 NaN   \n",
      "1  webpack learning academy exists to provide cur...  noinformationfound   \n",
      "2                                 noinformationfound  noinformationfound   \n",
      "3  Devops news, tutorials , infrastructure automa...  noinformationfound   \n",
      "4  Firebase is Google’s mobile platform that help...  noinformationfound   \n",
      "\n",
      "                                               htext  \\\n",
      "0  Top 5 Tutorials for Getting Started with React...   \n",
      "1  \\n          ©\\n          webpack learning acad...   \n",
      "2  \\n      Git is a free and open source\\n      d...   \n",
      "3  \\n\\n\\n\\n(adsbygoogle = window.adsbygoogle || [...   \n",
      "4  \\n            Firebase products like Analytics...   \n",
      "\n",
      "                             params  category  wiki  \\\n",
      "0      /getting-started-tutorials/        NaN   NaN   \n",
      "1             /p/the-core-concepts        NaN   NaN   \n",
      "2                   /docs/git-pull        NaN   NaN   \n",
      "3  /pluralsight-free-subscription/        NaN   NaN   \n",
      "4                        /pricing/        NaN   NaN   \n",
      "\n",
      "                                             alltext  jscheck  docscheck  \\\n",
      "0    react andrewhfarmer getting tutorials starte...        0          0   \n",
      "1    quality webpack high sean resource content s...        0          0   \n",
      "2    scm git version distributed small free proje...        0          0   \n",
      "3    cloud beginners news devopscube infrastructu...        0          0   \n",
      "4    pricing quality reporting database platform ...        0          0   \n",
      "\n",
      "   blogscheck  commcheck  \n",
      "0           0          0  \n",
      "1           0          0  \n",
      "2           0          0  \n",
      "3           0          0  \n",
      "4           0          0  \n",
      "(107, 15)\n",
      "ok\n",
      "ok\n",
      "You are to overwrite the following file : /home/ec/Documents/MainComp_Programming/FreeCodeCamp/basejumps/fccr3_da/data/notatedplatformsphase1_a7.csv.\n",
      "Is that correct?\n",
      "n\n",
      "Overwriting aborted.\n"
     ]
    }
   ],
   "source": [
    "pd_annotated, pd_notated = dataloadandprep(loading=(True,True),\n",
    "                                           status='7',\n",
    "                                           increasesizetrain=False,\n",
    "                                           wiki=(False,False),\n",
    "                                           prep=(False,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-d5af98942240>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-d5af98942240>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    STOP HERE!\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "STOP HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def vectorizer(documents):\n",
    "    countvect = CountVectorizer(ngram_range=(1,2))\n",
    "    X_counts = countvect.fit_transform(documents)\n",
    "    return X_counts, countvect\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "import copy\n",
    "def hstacker(X, hackedcolumns):\n",
    "    X_countsh = hstack((copy.copy(X), hackedcolumns))\n",
    "    return X_countsh\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "def modelling(Classifier, X, y, params = {}):\n",
    "    #clf_dtm = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1, random_state=0)\n",
    "    clf = Classifier()\n",
    "    if params != {}:\n",
    "        clf.set_params(**params)\n",
    "    return clf, clf.fit(X, y)\n",
    "\n",
    "def compareyvsypredict(target, y, y_predict):\n",
    "    print('{0:35} => {1:15} => {2:15}'.format('Target','Class','PredClass'))\n",
    "    for plat, category in zip(zip(target, y), y_predict):\n",
    "        print('{0:35} => {1:15} => {2:15}'.format(plat[0], plat[1], category))\n",
    "    \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "def gridsearchwithprecandrecall(model, X_train, y_train, X_test, y_test, tuned_parameters):\n",
    "    # Set the parameters by cross-validation\n",
    "    tuned_parameters = [tuned_parameters]\n",
    "    scores = ['precision', 'recall']\n",
    "\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(model, tuned_parameters, cv=2,\n",
    "                           scoring='%s_macro' % score)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        y_true, y_predsearch = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_predsearch))\n",
    "        print()\n",
    "    return clf\n",
    "\n",
    "    \n",
    "def err(clf_stump, clf_err, X_test, y_test, params={}):\n",
    "    if params != {}:\n",
    "        clf_err.set_params(**params)\n",
    "    clf_stump_err = 1.0 - clf_stump.score(X_test, y_test)\n",
    "    clf_err = 1.0 - clf_err.score(X_test, y_test)\n",
    "    print('stump error : {0:.2f}; model error : {1:.2f}'.format(clf_stump_err, clf_err))\n",
    "    return clf_stump_err, clf_err\n",
    "\n",
    "\n",
    "\n",
    "#IMPORTANT... https://stackoverflow.com/questions/33110973/pass-a-dict-to-scikit-learn-estimator\n",
    "#clf_dtm_r2.set_params(**clf.best_params_)\n",
    "#clf_dt_r2 = clf_dtm_r2.fit(X_primercounts2, train2.category)\n",
    "\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def ADA(model, stump_err, mod_err, X_train, y_train, X_test, y_test, n_estimators = 40, learning_rate = 1.):\n",
    "    ada_discrete = AdaBoostClassifier(\n",
    "        base_estimator = model,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        algorithm=\"SAMME\")\n",
    "    ada_discrete.fit(X_train, y_train)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.plot([1, n_estimators], [stump_err] * 2, 'k-',\n",
    "            label='Decision Stump Error')\n",
    "    ax.plot([1, n_estimators], [mod_err] * 2, 'k--',\n",
    "            label='Decision Tree Error')\n",
    "\n",
    "    ada_discrete_err = numpy.zeros((n_estimators,))\n",
    "    for i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n",
    "        ada_discrete_err[i] = zero_one_loss(y_pred, y_test)\n",
    "\n",
    "    ada_discrete_err_train = numpy.zeros((n_estimators,))\n",
    "    for i, y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n",
    "        ada_discrete_err_train[i] = zero_one_loss(y_pred, y_train)\n",
    "\n",
    "    ax.plot(numpy.arange(n_estimators) + 1, ada_discrete_err,\n",
    "            label='Discrete AdaBoost Test Error',\n",
    "            color='red')\n",
    "    ax.plot(numpy.arange(n_estimators) + 1, ada_discrete_err_train,\n",
    "            label='Discrete AdaBoost Train Error',\n",
    "            color='blue')\n",
    "\n",
    "    ax.set_ylim((0.0, 1.0))\n",
    "    ax.set_xlabel('n_estimators')\n",
    "    ax.set_ylabel('error rate')\n",
    "\n",
    "    leg = ax.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.7)\n",
    "\n",
    "    plt.show()\n",
    "    return ada_discrete\n",
    "\n",
    "\n",
    "def decisiontreeanalysis(X_train,  X_trainh, y_train, X_test, X_testh, y_test):\n",
    "    slump_params = {'max_depth':1, 'min_samples_leaf':1, 'random_state':0}\n",
    "    mod_params = {'min_samples_leaf':2, 'random_state':0}\n",
    "\n",
    "    clf_dt_slump, dt_fit_slump = modelling(DecisionTreeClassifier, X_train, y_train, slump_params)\n",
    "    clf_dt_mod, dt_fit = modelling(DecisionTreeClassifier, X_train, y_train, mod_params)\n",
    "\n",
    "    clf_dt_slumph, dt_fit_slumph = modelling(DecisionTreeClassifier, X_trainh, y_train, slump_params)\n",
    "    clf_dt_modh, dt_fith = modelling(DecisionTreeClassifier, X_trainh, y_train, mod_params)\n",
    "    \n",
    "    tuned_parameters = {'max_depth': numpy.arange(5,25,2)}\n",
    "    gs_clf_dt_mod = gridsearchwithprecandrecall(clf_dt_mod, X_train, y_train, X_test, y_test, tuned_parameters)\n",
    "    gs_clf_dt_modh = gridsearchwithprecandrecall(clf_dt_modh, X_trainh, y_train, X_testh, y_test, tuned_parameters)\n",
    "    \n",
    "    clf_dt_stump_err, clf_dt_mod_err = err(clf_dt_slump, clf_dt_mod, X_test, y_test, gs_clf_dt_mod.best_params_)\n",
    "    clf_dt_stump_errh, clf_dt_mod_errh = err(clf_dt_slumph, clf_dt_modh, X_testh, y_test, gs_clf_dt_modh.best_params_)\n",
    "    \n",
    "    return clf_dt_slump, dt_fit_slump, clf_dt_mod, dt_fit, clf_dt_slumph, dt_fit_slumph, clf_dt_modh, dt_fith, gs_clf_dt_mod, gs_clf_dt_modh, clf_dt_stump_err, clf_dt_mod_err, clf_dt_stump_errh, clf_dt_mod_errh\n",
    "\n",
    "\n",
    "\n",
    "def supportvectormachineanalysis(X_train,  X_trainh, y_train, X_test, X_testh, y_test):\n",
    "    slump_params = {'C':1.}\n",
    "    clf_svc_slump, svc_fit_slump = modelling(svm.LinearSVC, X_train, y_train, slump_params)\n",
    "    clf_svc_mod, svc_fit = modelling(svm.LinearSVC, X_train, y_train)\n",
    "\n",
    "    clf_svc_slumph, svc_fit_slumph = modelling(svm.LinearSVC, X_trainh, y_train, slump_params)\n",
    "    clf_svc_modh, svc_fith = modelling(svm.LinearSVC, X_trainh, y_train)\n",
    "\n",
    "    tuned_parameters = {'C': numpy.arange(.1,1,.1)}\n",
    "    gs_clf_svc_mod = gridsearchwithprecandrecall(clf_svc_mod, X_train, y_train, X_test, y_test, tuned_parameters)\n",
    "    gs_clf_svc_modh = gridsearchwithprecandrecall(clf_svc_modh, X_trainh, y_train, X_testh, y_test, tuned_parameters)\n",
    "    clf_svc_stump_err, clf_svc_mod_err = err(clf_svc_slump, clf_svc_mod, X_test, y_test, gs_clf_svc_mod.best_params_)\n",
    "    clf_svc_stump_errh, clf_svc_mod_errh = err(clf_svc_slumph, clf_svc_modh, X_testh, y_test, gs_clf_svc_modh.best_params_)\n",
    "\n",
    "    return clf_svc_slump, svc_fit_slump, clf_svc_mod, svc_fit, clf_svc_slumph, svc_fit_slumph, clf_svc_modh, svc_fith, gs_clf_svc_mod, gs_clf_svc_modh, clf_svc_stump_err, clf_svc_mod_err, clf_svc_stump_errh, clf_svc_mod_errh\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def cossim(train,tst,countmod,compare=True):\n",
    "    '''\n",
    "    tst : any test file added\n",
    "    reference:\n",
    "    --- https://stackoverflow.com/questions/14808945/check-if-variable-is-dataframe\n",
    "    '''\n",
    "    #print(tst.shape)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(train['category'])\n",
    "    print(le.classes_)\n",
    "    models = {}\n",
    "    for les in le.classes_:\n",
    "        yes = ' '.join(train.loc[train['category']==les,'alltext'])\n",
    "        no  = ' '.join(train.loc[train['category']!=les,'alltext'])\n",
    "        models[les] = (countmod.transform([yes]), countmod.transform([no]))\n",
    "    \n",
    "    quickanal = []\n",
    "    assigneds = []\n",
    "    for rec in tst.loc[:,['category','alltext']].iterrows():\n",
    "        m = countmod.transform([rec[1].alltext])\n",
    "        vals = []\n",
    "        for les in le.classes_:\n",
    "            vals.append(cosine_similarity(m, models[les][0]))\n",
    "        index = vals.index(max(vals))\n",
    "        assigned = le.classes_[index]\n",
    "        #print('Category was {0}; Assigned was {1} (cosine_sim : {2:.2f})'.format(rec[1].category,assigned,max(vals)))\n",
    "        if compare:\n",
    "            print('cosine similary ',rec[1].category,assigned,max(vals))\n",
    "            if rec[1].category == assigned:\n",
    "                quickanal.append(1)\n",
    "            else:\n",
    "                quickanal.append(0)\n",
    "        assigneds.append(assigned)\n",
    "        #break\n",
    "    \n",
    "    if compare:\n",
    "        print(sum(quickanal)/len(quickanal))\n",
    "        print(classification_report(tst['category'], assigneds))\n",
    "    \n",
    "    return assigneds\n",
    "\n",
    "\n",
    "\n",
    "def aggregatesclasses(test, X_test, X_testh, y_test, clf_adadt_mod, clf_dt_modh, clf_svc_modh, assigneds, compare = True):    \n",
    "    \n",
    "    def funcfact(func, args):\n",
    "        return func.predict(**args)\n",
    "    \n",
    "    def classagg(x):\n",
    "        if x.pred_dtada == x.pred_dth == x.pred_svch and x.pred_dth != 'TRAINING':\n",
    "            return x.pred_dtada\n",
    "        return x.pred_svch\n",
    "\n",
    "    def classagg1(x):\n",
    "        if x.pred_dth == x.pred_svch and x.pred_dth != 'TRAINING':\n",
    "            return x.pred_dth\n",
    "        if x.pred_dtada == x.pred_dth and x.pred_dth != 'TRAINING':\n",
    "            return x.pred_dtada\n",
    "        if x.pred_dtada == x.pred_svch and x.pred_dth != 'TRAINING':\n",
    "            return x.pred_dtada\n",
    "        return x.pred_svch\n",
    "\n",
    "    def classagg2(x):\n",
    "        if x.pred_dth == x.pred_svch and x.pred_dth != 'TRAINING':\n",
    "            return x.pred_dth\n",
    "        if x.pred_dtada == x.pred_dth and x.pred_dth != 'TRAINING':\n",
    "            return x.pred_dtada\n",
    "        if x.pred_dtada == x.pred_svch and x.pred_dth != 'TRAINING':\n",
    "            return x.pred_dtada\n",
    "        return x.pred_dth\n",
    "\n",
    "    def classagg3(x):\n",
    "        def value(args):\n",
    "            cv = args[0]\n",
    "            maxim = len(args)\n",
    "            val = 1\n",
    "            for i in args[1:]:\n",
    "                if cv == i:\n",
    "                    val += 1\n",
    "                else:\n",
    "                    break\n",
    "            if val == maxim:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "        if x.cossim in ['DOCS','REPL','BUSINESS','ECOMMERCE','COMMUITY','SENGINE','PAAS'] and not value([x.pred_dtada, x.pred_dth, x.pred_svch]):\n",
    "            return x.cossim\n",
    "\n",
    "        if x.pred_dth == x.cossim in ['TRAINING','NEWS','PACKAGE','NOCLASS','THEME']:\n",
    "            return x.pred_dth\n",
    "\n",
    "        #if x.pred_dtada == x.pred_dth == x.pred_svch == x.cossim and x.pred_dth != 'TRAINING':\n",
    "        if x.pred_dtada == x.pred_dth == x.pred_svch == x.cossim:\n",
    "            return x.pred_dtada\n",
    "\n",
    "        return x.pred_dth\n",
    "    \n",
    "    if compare:\n",
    "        pd_predaggregate = pandas.DataFrame({'platform':test.platform, 'category':y_test, 'pred_dtada':clf_adadt_mod.predict(X_test), 'pred_dth':clf_dt_modh.predict(X_testh), 'pred_svch':clf_svc_modh.predict(X_testh), 'cossim':assigneds})\n",
    "    \n",
    "        print('pred_dtada')\n",
    "        print(classification_report(pd_predaggregate['category'], pd_predaggregate['pred_dtada']))\n",
    "        print('pred_dth')\n",
    "        print(classification_report(pd_predaggregate['category'], pd_predaggregate['pred_dth']))\n",
    "        print('pred_svch')\n",
    "        print(classification_report(pd_predaggregate['category'], pd_predaggregate['pred_svch']))\n",
    "        print('cossim')\n",
    "        print(classification_report(pd_predaggregate['category'], pd_predaggregate['cossim']))\n",
    "        print('pred_agg1')\n",
    "        pd_predaggregate['pred_agg1'] = ''\n",
    "        pd_predaggregate['pred_agg1'] = pd_predaggregate.apply(classagg1, axis=1)\n",
    "        print(classification_report(pd_predaggregate['category'], pd_predaggregate['pred_agg1']))\n",
    "        print('pred_agg2')\n",
    "        pd_predaggregate['pred_agg2'] = ''\n",
    "        pd_predaggregate['pred_agg2'] = pd_predaggregate.apply(classagg2, axis=1)        \n",
    "        print(classification_report(pd_predaggregate['category'], pd_predaggregate['pred_agg2']))\n",
    "        print('pred_agg3')\n",
    "        pd_predaggregate['pred_agg3'] = ''\n",
    "        pd_predaggregate['pred_agg3'] = pd_predaggregate.apply(classagg3, axis=1)           \n",
    "        print(classification_report(pd_predaggregate['category'], pd_predaggregate['pred_agg3']))\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        print(len(test.platform), len(clf_adadt_mod.predict(X_test)), len(clf_dt_modh.predict(X_testh)), len(clf_svc_modh.predict(X_testh)), len(assigneds))\n",
    "        pd_predaggregate = pandas.DataFrame({'platform':test.platform, 'pred_dtada':clf_adadt_mod.predict(X_test), 'pred_dth':clf_dt_modh.predict(X_testh), 'pred_svch':clf_svc_modh.predict(X_testh), 'cossim':assigneds})\n",
    "        pd_predaggregate['pred_agg1'] = ''\n",
    "        pd_predaggregate['pred_agg1'] = pd_predaggregate.apply(classagg1, axis=1)\n",
    "        pd_predaggregate['pred_agg2'] = ''\n",
    "        pd_predaggregate['pred_agg2'] = pd_predaggregate.apply(classagg2, axis=1)   \n",
    "        pd_predaggregate['pred_agg3'] = ''\n",
    "        pd_predaggregate['pred_agg3'] = pd_predaggregate.apply(classagg3, axis=1)   \n",
    "        return pd_predaggregate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-d5af98942240>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-d5af98942240>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    STOP HERE!\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "STOP HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def datafullprocessing(pdfiles, nextstatus = '', create_unlab = False):\n",
    "    '''\n",
    "    pd_anndata = pdfiles[0]\n",
    "    pd_noandata = pdfiles[1]\n",
    "    '''\n",
    "    train=pdfiles[0][['platform', 'alltext', 'jscheck', 'docscheck', 'blogscheck', 'commcheck', 'category']].sample(frac=0.75)\n",
    "    test=pdfiles[0][['platform', 'alltext','jscheck', 'docscheck', 'blogscheck', 'commcheck', 'category']].drop(train.index)\n",
    "    X_train, countmod = vectorizer(train.alltext)\n",
    "    X_trainh = hstacker(X_train, train[['jscheck', 'docscheck', 'blogscheck', 'commcheck']])\n",
    "    y_train = train.category\n",
    "    def testf(test):\n",
    "        X_test = countmod.transform(test.alltext)\n",
    "        X_testh = hstacker(X_test, test[['jscheck', 'docscheck', 'blogscheck', 'commcheck']])\n",
    "        y_test = test.category\n",
    "        return X_test, X_testh, y_test\n",
    "    X_test, X_testh, y_test = testf(test)\n",
    "    if create_unlab:\n",
    "        unltest = pdfiles[1][['platform', 'alltext', 'jscheck', 'docscheck', 'blogscheck', 'commcheck', 'category']]\n",
    "        X_unltest, X_unltesth, y_unltest = testf(unltest)\n",
    "\n",
    "    ##decision tree\n",
    "    clf_dt_slump, dt_fit_slump, clf_dt_mod, dt_fit, clf_dt_slumph, dt_fit_slumph, clf_dt_modh, dt_fith, gs_clf_dt_mod, gs_clf_dt_modh, clf_dt_stump_err, clf_dt_mod_err, clf_dt_stump_errh, clf_dt_mod_errh = decisiontreeanalysis(X_train,  X_trainh, y_train, X_test, X_testh, y_test)\n",
    "\n",
    "    ##adaboost for decision tree\n",
    "    clf_adadt_mod = ADA(clf_dt_mod, clf_dt_stump_err, clf_dt_mod_err, X_train, y_train, X_test, y_test, n_estimators = 40, learning_rate = .5)\n",
    "    clf_adadt_modh = ADA(clf_dt_modh, clf_dt_stump_errh, clf_dt_mod_errh, X_trainh, y_train, X_testh, y_test, n_estimators = 40, learning_rate = .5)\n",
    "    \n",
    "    ##linear support vector machine\n",
    "    clf_svc_slump, svc_fit_slump, clf_svc_mod, svc_fit, clf_svc_slumph, svc_fit_slumph, clf_svc_modh, svc_fith, gs_clf_svc_mod, gs_clf_svc_modh, clf_svc_stump_err, clf_svc_mod_err, clf_svc_stump_errh, clf_svc_mod_errh = supportvectormachineanalysis(X_train,  X_trainh, y_train, X_test, X_testh, y_test)\n",
    "    \n",
    "    assigneds = cossim(train, test, countmod)\n",
    "\n",
    "    aggregatesclasses(test, X_test, X_testh, y_test, clf_adadt_mod, clf_dt_modh, clf_svc_modh, assigneds)\n",
    "    \n",
    "    if create_unlab:\n",
    "        print('This is shape of unlabelled ', unltest.shape)\n",
    "        unlassigneds = cossim(train, unltest, countmod, compare=False)\n",
    "        unlabelled = aggregatesclasses(unltest, X_unltest, X_unltesth, None, clf_adadt_mod, clf_dt_modh, clf_svc_modh, unlassigneds, compare=False)\n",
    "        print('unlabelled')\n",
    "        print(unlabelled)\n",
    "        datasaving(unlabelled, form = 'unlabelled', status = nextstatus)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA ANALYSIS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAPTURING NOTANNOTATED DATA FOR THE FIRST TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the data and overwriting the nonannotated data with new details (wiki + prepr.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfiles = dataloadandprep(loading=(True,True),\n",
    "                status='7',\n",
    "                increasesizetrain=False,\n",
    "                wiki=(False,False),\n",
    "                prep=(False,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE NEXT UNLABELLED FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datafullprocessing(pdfiles, nextstatus = '8', create_unlab = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datafullprocessing([pd_annotated, pd_notated], create_unlab = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUND 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST- MANUAL LABELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the size of the annotated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1                   platform  \\\n",
      "0           0             0  v4-alpha.getbootstrap.com   \n",
      "1           1             1                jscroll.com   \n",
      "2           2             2             mongoosejs.com   \n",
      "3           3             3         www.highcharts.com   \n",
      "4           4             4         www.crossorigin.me   \n",
      "\n",
      "                                               title  \\\n",
      "0  \\n  \\n    bootstrap · the most popular html, c...   \n",
      "1  jscroll - a jquery plugin for infinite scrolli...   \n",
      "2                               mongoose odm v4.10.5   \n",
      "3  Interactive JavaScript charts for your webpage...   \n",
      "4                                     crossorigin.me   \n",
      "\n",
      "                                         description  \\\n",
      "0  the most popular html, css, and js framework i...   \n",
      "1                                                      \n",
      "2                                                      \n",
      "3  Highcharts - Interactive JavaScript charts for...   \n",
      "4                                                      \n",
      "\n",
      "                                            keywords  \\\n",
      "0                                                      \n",
      "1                                                      \n",
      "2                                                      \n",
      "3  highcharts, charts, javascript charts, ajax ch...   \n",
      "4                                                      \n",
      "\n",
      "                                               htext  \\\n",
      "0  Bootstrap is the most popular HTML, CSS, and J...   \n",
      "1  jScroll is a jQuery plugin for infinite scroll...   \n",
      "2  Elegant MongoDB object modeling for Node.js Mo...   \n",
      "3                           View demo Get a license    \n",
      "4  \\n                  Welcome to crossorigin.me,...   \n",
      "\n",
      "                                              params wiki  \\\n",
      "0  ,/components,/components/breadcrumb/,/migratio...  NaN   \n",
      "1                 /,https://github.com/cubiq/iscroll  NaN   \n",
      "2  /docs/promises.html,/docs/populate.html,/docs/...  NaN   \n",
      "3  /samples/data/jsonp.php,/blog/192-use-highchar...  NaN   \n",
      "4  /https://en.wikipedia.org/w/api.php,/,/https:/...  NaN   \n",
      "\n",
      "                                             alltext  jscheck  docscheck  \\\n",
      "0    components clearfix world carousel buttons h...        0          0   \n",
      "1    plugin loaded github new com iscroll cubiq e...        0          0   \n",
      "2    validation mongoose models node queries api ...        1          0   \n",
      "3    charts samples jsonp navigator view highchar...        0          0   \n",
      "4    crossorigin org wikipedia api w free https e...        0          0   \n",
      "\n",
      "   blogscheck  commcheck category  \n",
      "0           0          0  PACKAGE  \n",
      "1           0          0  PACKAGE  \n",
      "2           0          0  PACKAGE  \n",
      "3           0          0  PACKAGE  \n",
      "4           0          0  PACKAGE  \n",
      "(1096, 15)\n",
      "   Unnamed: 0  Unnamed: 0.1             platform  \\\n",
      "0           0             0    andrewhfarmer.com   \n",
      "1           1             1      webpack.academy   \n",
      "2           2             2          git-scm.com   \n",
      "3           3             3       devopscube.com   \n",
      "4           4             4  firebase.google.com   \n",
      "\n",
      "                                               title  \\\n",
      "0                                                NaN   \n",
      "1                    Home | webpack learning academy   \n",
      "2                                                Git   \n",
      "3  DevopsCube - DevOps trends, News, tutorials, c...   \n",
      "4                                           Firebase   \n",
      "\n",
      "                                         description            keywords  \\\n",
      "0                                                NaN                 NaN   \n",
      "1  webpack learning academy exists to provide cur...  noinformationfound   \n",
      "2                                 noinformationfound  noinformationfound   \n",
      "3  Devops news, tutorials , infrastructure automa...  noinformationfound   \n",
      "4  Firebase is Google’s mobile platform that help...  noinformationfound   \n",
      "\n",
      "                                               htext  \\\n",
      "0  Top 5 Tutorials for Getting Started with React...   \n",
      "1  \\n          ©\\n          webpack learning acad...   \n",
      "2  \\n      Git is a free and open source\\n      d...   \n",
      "3  \\n\\n\\n\\n(adsbygoogle = window.adsbygoogle || [...   \n",
      "4  \\n            Firebase products like Analytics...   \n",
      "\n",
      "                             params  category  wiki  \\\n",
      "0      /getting-started-tutorials/        NaN   NaN   \n",
      "1             /p/the-core-concepts        NaN   NaN   \n",
      "2                   /docs/git-pull        NaN   NaN   \n",
      "3  /pluralsight-free-subscription/        NaN   NaN   \n",
      "4                        /pricing/        NaN   NaN   \n",
      "\n",
      "                                             alltext  jscheck  docscheck  \\\n",
      "0    react andrewhfarmer getting tutorials starte...        0          0   \n",
      "1    quality webpack high sean resource content s...        0          0   \n",
      "2    scm git version distributed small free proje...        0          0   \n",
      "3    cloud beginners news devopscube infrastructu...        0          0   \n",
      "4    pricing quality reporting database platform ...        0          0   \n",
      "\n",
      "   blogscheck  commcheck  \n",
      "0           0          0  \n",
      "1           0          0  \n",
      "2           0          0  \n",
      "3           0          0  \n",
      "4           0          0  \n",
      "(107, 15)\n",
      "Could open  /home/ec/Documents/MainComp_Programming/FreeCodeCamp/basejumps/fccr3_da/data/unlabelledplatformsphase1_a7.csv successfully\n",
      "Index(['Unnamed: 0', 'cossim', 'platform', 'pred_dtada', 'pred_dth',\n",
      "       'pred_svch', 'pred_agg1', 'pred_agg2', 'pred_agg3', 'checked',\n",
      "       'category'],\n",
      "      dtype='object') (107, 11)\n",
      "(107, 15) Index(['Unnamed: 0', 'Unnamed: 0.1', 'platform', 'title', 'description',\n",
      "       'keywords', 'htext', 'params', 'wiki', 'alltext', 'jscheck',\n",
      "       'docscheck', 'blogscheck', 'commcheck', 'category'],\n",
      "      dtype='object') 0      TRAINING\n",
      "1      TRAINING\n",
      "2       PACKAGE\n",
      "3          NEWS\n",
      "4          PAAS\n",
      "5      TRAINING\n",
      "6      TRAINING\n",
      "7       PACKAGE\n",
      "8       PACKAGE\n",
      "9          DOCS\n",
      "10         NEWS\n",
      "11         PAAS\n",
      "12         NEWS\n",
      "13         NEWS\n",
      "14      PACKAGE\n",
      "15      NOCLASS\n",
      "16     BUSINESS\n",
      "17      PACKAGE\n",
      "18         NEWS\n",
      "19         NEWS\n",
      "20     BUSINESS\n",
      "21      PACKAGE\n",
      "22      PACKAGE\n",
      "23     TRAINING\n",
      "24      NOCLASS\n",
      "25      PACKAGE\n",
      "26         PAAS\n",
      "27      NOCLASS\n",
      "28      PACKAGE\n",
      "29      SENGINE\n",
      "         ...   \n",
      "77         NEWS\n",
      "78     TRAINING\n",
      "79         NEWS\n",
      "80      PACKAGE\n",
      "81     COMMUITY\n",
      "82         NEWS\n",
      "83      NOCLASS\n",
      "84      PACKAGE\n",
      "85      PACKAGE\n",
      "86     TRAINING\n",
      "87      NOCLASS\n",
      "88     TRAINING\n",
      "89         REPL\n",
      "90      SENGINE\n",
      "91        THEME\n",
      "92      PACKAGE\n",
      "93      PACKAGE\n",
      "94         NEWS\n",
      "95      PACKAGE\n",
      "96      NOCLASS\n",
      "97     TRAINING\n",
      "98     BUSINESS\n",
      "99         NEWS\n",
      "100     PACKAGE\n",
      "101        DOCS\n",
      "102        DOCS\n",
      "103     PACKAGE\n",
      "104    COMMUITY\n",
      "105        DOCS\n",
      "106        NEWS\n",
      "Name: category, dtype: object\n",
      "(1203, 15) Index(['Unnamed: 0', 'Unnamed: 0.1', 'platform', 'title', 'description',\n",
      "       'keywords', 'htext', 'params', 'wiki', 'alltext', 'jscheck',\n",
      "       'docscheck', 'blogscheck', 'commcheck', 'category'],\n",
      "      dtype='object')\n",
      "Would you like to write to file to /home/ec/Documents/MainComp_Programming/FreeCodeCamp/basejumps/fccr3_da/data/annotatedplatformsphase1_a8.csv?\n",
      "yes\n",
      "The file :  annotatedplatformsphase1_a8 has been successfully saved. Please check.\n"
     ]
    }
   ],
   "source": [
    "dataloadandprep(loading=(True,True),\n",
    "                status='7',\n",
    "                increasesizetrain=True,\n",
    "                wiki=(False,False),\n",
    "                prep=(False,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-analysis of the extended annotated file and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdfiles = dataloadandprep(loading=(True,False),\n",
    "                status='8',\n",
    "                increasesizetrain=False,\n",
    "                wiki=(False,False),\n",
    "                prep=(False,False))\n",
    "\n",
    "datafullprocessing(pdfiles, create_unlab = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAPTURING NOTANNOTATED DATA FOR THE FIRST TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the data and overwriting the nonannotated data with new details (wiki + prepr.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfiles = dataloadandprep(loading=(True,True),\n",
    "                status='3',\n",
    "                increasesizetrain=False,\n",
    "                wiki=(False,True),\n",
    "                prep=(False,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE NEXT UNLABELLED FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datafullprocessing(pdfiles, nextstatus = '3', create_unlab = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUND 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST- MANUAL LABELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the size of the annotated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataloadandprep(loading=(True,True),\n",
    "                status='3',\n",
    "                increasesizetrain=True,\n",
    "                wiki=(False,False),\n",
    "                prep=(False,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-analysis of the extended annotated file and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdfiles = dataloadandprep(loading=(True,False),\n",
    "                status='4',\n",
    "                increasesizetrain=False,\n",
    "                wiki=(False,False),\n",
    "                prep=(False,False))\n",
    "\n",
    "datafullprocessing(pdfiles, create_unlab = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAPTURING NOTANNOTATED DATA FOR THE FIRST TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the data and overwriting the nonannotated data with new details (wiki + prepr.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfiles = dataloadandprep(loading=(True,True),\n",
    "                status='4',\n",
    "                increasesizetrain=False,\n",
    "                wiki=(False,True),\n",
    "                prep=(False,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE NEXT UNLABELLED FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datafullprocessing(pdfiles, nextstatus = '4', create_unlab = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUND 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST- MANUAL LABELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the size of the annotated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataloadandprep(loading=(True,True),\n",
    "                status='4',\n",
    "                increasesizetrain=True,\n",
    "                wiki=(False,False),\n",
    "                prep=(False,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-analysis of the extended annotated file and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdfiles = dataloadandprep(loading=(True,False),\n",
    "                status='5',\n",
    "                increasesizetrain=False,\n",
    "                wiki=(False,False),\n",
    "                prep=(False,False))\n",
    "\n",
    "datafullprocessing(pdfiles, create_unlab = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAPTURING NOTANNOTATED DATA FOR THE FIRST TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the data and overwriting the nonannotated data with new details (wiki + prepr.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfiles = dataloadandprep(loading=(True,True),\n",
    "                status='5',\n",
    "                increasesizetrain=False,\n",
    "                wiki=(False,True),\n",
    "                prep=(False,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE NEXT UNLABELLED FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datafullprocessing(pdfiles, nextstatus = '5', create_unlab = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUND 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST- MANUAL LABELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the size of the annotated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataloadandprep(loading=(True,True),\n",
    "                status='5',\n",
    "                increasesizetrain=True,\n",
    "                wiki=(False,False),\n",
    "                prep=(False,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-analysis of the extended annotated file and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdfiles = dataloadandprep(loading=(True,False),\n",
    "                status='6',\n",
    "                increasesizetrain=False,\n",
    "                wiki=(False,False),\n",
    "                prep=(False,False))\n",
    "\n",
    "datafullprocessing(pdfiles, create_unlab = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAPTURING NOTANNOTATED DATA FOR THE FIRST TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the data and overwriting the nonannotated data with new details (wiki + prepr.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfiles = dataloadandprep(loading=(True,True),\n",
    "                status='6',\n",
    "                increasesizetrain=False,\n",
    "                wiki=(False,True),\n",
    "                prep=(False,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE NEXT UNLABELLED FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datafullprocessing(pdfiles, nextstatus = '6', create_unlab = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
