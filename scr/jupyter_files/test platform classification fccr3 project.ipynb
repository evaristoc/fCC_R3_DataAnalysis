{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1: Platform Classification using Supervised Learning Methods - Building Training and Test datasets (part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is being used to test supervised learning methods to classify the platforms (ie. the resources commented on freeCodeCamp social media).\n",
    "\n",
    "The script has been mainly used to start the construction of an unexistent labelled dataset that could be eventually used for training and validation. Simultaneously it is being used to have some idea of the feasibility of a ML approach for classification, as well as to compare some methodologies that could be used in full implementation later on.\n",
    "\n",
    "## METHOD\n",
    "\n",
    "### The Categories\n",
    "\n",
    "The categories were a construct that seemed to respond to the main types of material consulted and shared by users in the main file. However, the resulting categories were identified *after* the selection/inclusion rules of resources (see next) so it is biased to that procedure.\n",
    "\n",
    "The categories are curated and can be consulted in the following folder:\n",
    "\n",
    "* https://github.com/evaristoc/fCC_R3_DataAnalysis/tree/development/docs\n",
    "\n",
    "### Data Preparation: The primer labelled file\n",
    "\n",
    "First source of labelled data came from the application of hard-coded conditional rules based on regex () or url domain words (eg. \"api\" for api, [\"devs\", \"docs\", etc.] for docs, [\"forum\", \"chat\", etc] for community). The script with the hard code rules might confuse the reader and therefore won't be attached to this script. There were also rules to pre-select the kind of platforms to be treated. Some of the rules were:\n",
    "\n",
    "* No gitter, memes, youtube, github, codepen, freecodecamp.com were included. The list of exclusion is longer.\n",
    "* Also excluded were those that ended in image format, like \\*.gif, or were scripts (eg. \\*.js).\n",
    "* From those that passed, those referring to topics like javascript, react, angular and others were included. The list of inclusions is longer.\n",
    "\n",
    "The accuracy of the hard coded rules procedure ended up around 60% after re-assigning classes based on personal judgement and comparing the resulting classifications.\n",
    "\n",
    "Data used for classification were based on extracting information through a bot from the main page of each resource and complemented with existing wikipedia sources about the resource. Data was not complete from all the resources and the text length and reliability of the information varied.\n",
    "\n",
    "The name of the file with the first revised classifications is `primer_classes_rev.csv`. This file was used as primer for further classifications.\n",
    "\n",
    "\n",
    "### Analysis\n",
    "\n",
    "The supervised methods tested so far were **Multinomial Na√Øve Bayesian (MNB)** and **Decision Trees (DT)**. Their selection was motivated by their simplicity and expected robustness when compared to more complex methods. A simple robust method was preferred because the small amount of data available. They are also frequenly used for document classification (https://en.wikipedia.org/wiki/Document_classification). I am also preferring a classifier for this case that could cope with an assumed nonlinear distribution.\n",
    "\n",
    "The incremental construction consisted in using a small training dataset for predicting the classes of unobserved records using either MNB or DT, verifying then the classes by inspecting resources online, assigning a class to a sample of records (personal judgement), evaluating accuracy, and finally increasing the size of the training dataset with the newly annotated data to classify records that were not added to the sample. Then repeating the process until an arbitrary size.\n",
    "\n",
    "The analyses were very simple and focused in simple calculations of accuracy and precision in spreadsheets, used to compare predicted classes vs the assigned classes at each iteration.\n",
    "**Be aware that** because of part of the activities were made in spreadsheets and didn't use a script, the procedure is unfortunately not fully recorded and some steps are missing. Not all the files are provided either.\n",
    "\n",
    "## RESULTS so far...\n",
    "\n",
    "The accuracy of the classification has been between 50%-60%, better for DT than for MNB. Accuracy was acceptable for this phase and amount of data, very close to the hard code rule implementation, but probably insufficient for an stand-alone implementation, requiring manual supervision. Precision of certain classes have also been analysed, being relatively high for a few of them. Recall seemed to be very low in all steps.\n",
    "\n",
    "Another important result was that MNB tended to bias to the most numerous categories. Some tuning of the alpha-parameter of MNB was tried but the results at this stage were not useful.\n",
    "\n",
    "\n",
    "## OBSERVATIONS\n",
    "\n",
    "One thing worth mentioning is that the classification is becoming *highly unbalanced*.\n",
    "\n",
    "One reason is that I am using my own judgement for the classification: although some platforms are easy to classify there are some of them that are between two classifications.\n",
    "\n",
    "The other reason is that in fact the selection of the users felt mostly in certain type of resources. Blogs for example comprise a large part of the resources that users consult and mention. Similarly, it has to do with the actual distribution of the different types of platforms on the web: for example it is very likely that the number of bloggers is much bigger than the number of online learning platforms that actually exist, so even if the user wanted to find more learning platforms than blogging sites, that would not be possible because it doesn't reflects the actual distribution of site types on Internet.\n",
    "\n",
    "Another challenge of this project has been that those kind of resources might have been used differently per Gitter channel. Some channels might rely more on guides while others were more busy consulting and sharing frameworks or api's.\n",
    "\n",
    "**Be also aware that** the code below is not following a strict, validated methodology. One reason is the small amount of data available when the tests started (10/7/2017). This is also a preliminary test of a methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "from IPython.display import display, Math, Latex #also '%%latex' magic command\n",
    "import collections, itertools, operator, re, copy, datetime\n",
    "import urllib, urllib.request, urllib.parse, dns, ipwhois\n",
    "import pickle, json, csv, zipfile\n",
    "import math, random, numpy, scipy, pandas\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import bs4\n",
    "import nltk, sklearn\n",
    "\n",
    "nltk.data.path.append(config.anacondadir+'nltk_data') #an unfortunate hack for now... need to create a relative link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "try:\n",
    "    imp.find_module('bs4')\n",
    "    found = True\n",
    "except ImportError:\n",
    "    found = False\n",
    "\n",
    "found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "* https://stackoverflow.com/questions/82831/how-do-i-check-whether-a-file-exists-using-python\n",
    "* https://stackoverflow.com/questions/14050281/how-to-check-if-a-python-module-exists-without-importing-it\n",
    "* http://www.dnspython.org/examples.html\n",
    "* https://stackoverflow.com/questions/24580373/how-to-get-whois-info-by-ip-in-python-3\n",
    "\n",
    "* http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "* https://web.stanford.edu/class/cs124/lec/naivebayes.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation 1 : primer TRAINING DATASET created using hard coded rules over Gitter HelpBackEnd chatroom (Jun-16 / Mar-17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING primer AND ADDING AND PARSING BOT AND WIKIPEDIA TEXT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = config.directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not pathlib.Path(directory+'primer_classes_rev.csv').is_file():\n",
    "    with open(directory+'primer_classes_rev.csv','w') as outfile:\n",
    "        csvfile = csv.writer(outfile)\n",
    "        csvfile.writerow(['platform','class'])\n",
    "        for k,v in backendclass.items():\n",
    "            csvfile.writerow([k,v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if pathlib.Path(directory+'primer_classes_rev.csv').is_file():\n",
    "    pd_primerclass = pandas.read_csv(open(directory+'primer_classes_rev.csv', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdbackendclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare this one to save data into the pandas dataset\n",
    "#a merge would have been MUUUUUUUUUCH better -- change this for a merge instead!\n",
    "def getting_treated_links(data, filename):\n",
    "    platformlist = data['platform']\n",
    "    with open(directory+filename+'_platforms_data.pkl','br') as infile:\n",
    "        botdata = pickle.load(infile)\n",
    "        #print(crawled)\n",
    "        for k1 in list(botdata.keys()):\n",
    "            if k1 in list(platformlist):\n",
    "                if botdata[k1]['title'] == -1 or botdata[k1]['title'] == 0 or botdata[k1]['title'] == None:\n",
    "                    data.loc[data['platform'] == k1,'title'] = ''\n",
    "                else:\n",
    "                    data.loc[data['platform'] == k1,'title'] = botdata[k1]['title']\n",
    "                if botdata[k1]['description'] == -1 or botdata[k1]['description'] == 0 or botdata[k1]['description'] == None:\n",
    "                    data.loc[data['platform'] == k1,'description'] = ''\n",
    "                else:\n",
    "                    data.loc[data['platform'] == k1,'description'] = botdata[k1]['description']\n",
    "                if botdata[k1]['keywords'] == -1 or botdata[k1]['keywords'] == 0 or botdata[k1]['keywords'] == None:\n",
    "                    data.loc[data['platform'] == k1,'keywords'] = ''\n",
    "                else:\n",
    "                    data.loc[data['platform'] == k1,'keywords'] = botdata[k1]['keywords']\n",
    "                if botdata[k1]['htext'] == -1 or botdata[k1]['htext'] == 0 or botdata[k1]['htext'] == None:\n",
    "                    data.loc[data['platform'] == k1,'htext'] = ''\n",
    "                else:\n",
    "                    data.loc[data['platform'] == k1,'htext'] = botdata[k1]['htext']\n",
    "                if botdata[k1]['params'] == -1 or botdata[k1]['params'] == 0 or botdata[k1]['params'] == None:\n",
    "                    data.loc[data['platform'] == k1,'params'] = ''\n",
    "                else:\n",
    "                    data.loc[data['platform'] == k1,'params'] = \",\".join([x for x in botdata[k1][\"params\"]])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_primerclass.query(\"'techboyzzz.wordpress.com' in platform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_primerclass[pd_primerclass['platform'] == 'techboyzzz.wordpress.com']['newclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_primerclass['title'] = ''\n",
    "pd_primerclass['description'] = ''\n",
    "pd_primerclass['keywords'] = ''\n",
    "pd_primerclass['htext'] = ''\n",
    "pd_primerclass['params'] = ''\n",
    "\n",
    "pb_platformsdata1 = getting_treated_links(pd_primerclass, 'helpbackend1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pb_platformsdata1.loc[pb_platformsdata1['platform'] == 'techboyzzz.wordpress.com',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wikipediasearch(platform):\n",
    "    title = ''\n",
    "    while True:\n",
    "        url = 'https://en.wikipedia.org/w/api.php?action=query&list=search&format=json&srsearch='+platform\n",
    "        req = urllib.request.Request(url)\n",
    "        resp = urllib.request.urlopen(req)\n",
    "        respData = resp.read()\n",
    "        r = json.loads(respData.decode(\"utf-8\"))\n",
    "        if 'error' in list(r.keys()):\n",
    "            return title\n",
    "        if r['query']['search'] != []:\n",
    "            break\n",
    "        elif r['query']['search'] == [] and len(platform.split('.')) > 2:\n",
    "            platform = '.'.join(platform.split('.')[1:])\n",
    "        elif r['query']['search'] == [] and len(platform.split('.')) <= 2:\n",
    "            print(platform, ' not found in wikipedia')\n",
    "            break\n",
    "    for i,t in  enumerate(r['query']['search']):\n",
    "        if set(t['title'].lower().replace('.', ' ').split(' ')).intersection(set(platform.split('.'))):\n",
    "            title = t['title']\n",
    "            break\n",
    "        elif set([''.join(t['title'].lower().replace('.', ' ').split(' '))]).intersection(set(platform.split('.'))):\n",
    "            title = t['title']\n",
    "            break\n",
    "    \n",
    "    print(platform, title)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wikipediaextract(title):\n",
    "    title = title.replace(' ', '%20')\n",
    "    url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&titles='+title\n",
    "    req = urllib.request.Request(url)\n",
    "    resp = urllib.request.urlopen(req)\n",
    "    respData = resp.read()\n",
    "    r = json.loads(respData.decode(\"utf-8\"))\n",
    "    #print(r)\n",
    "    return list(r['query']['pages'].values())[0]['extract']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def souping(extract):\n",
    "    soup = bs4.BeautifulSoup(extract)\n",
    "    print(soup.find_all('p')[0].text)\n",
    "    return soup.find_all('p')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdbackendclass['wiki'] = ''\n",
    "\n",
    "def getting_wikipedia(data):\n",
    "    for plt in data['platform']:\n",
    "        title = wikipediasearch(plt)\n",
    "        print(title, ' in getting wikipedia')\n",
    "        if title == '':\n",
    "            data.loc[data['platform'] == plt,'wiki'] = ''\n",
    "            continue\n",
    "        extract = wikipediaextract(title)\n",
    "        wiki = souping(extract)\n",
    "        #print(wiki)\n",
    "        data.loc[data['platform'] == plt,'wiki'] = wiki\n",
    "        #data.loc[data['platform'] == plt,'wiki'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getting_wikipedia(pd_primerclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_primerclass.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test = 'heroku.com'\n",
    "#reg = []\n",
    "#for rdata in dns.resolver.query(test):\n",
    "#    reg.append(rdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for rd in reg:\n",
    "#    obj = ipwhois.IPWhois(rdata)\n",
    "#    res=obj.lookup()\n",
    "#    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pdbackendclass['alltext'] = pdbackendclass['wiki'] + ' ' + pdbackendclass['title'] + ' ' + pdbackendclass['description'] + ' ' + pdbackendclass['keywords'] + ' ' + pdbackendclass['htext']\n",
    "#pdbackendclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def datapreparation(data):\n",
    "    \n",
    "    usual_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    other_words = [\"re\", \"fm\", \"tv\", \"la\", \"al\", \"ben\", \"aq\", \"ca\", \"can\", \"can'\", \"can't\", \"cant\", \"&\"]\n",
    "    punctuation = [\"\\\\\",\"/\", \"|\",\"(\",\")\",\".\",\",\",\":\",\"=\",\"{\",\"}\",\"==\", \"===\",\"[\",\"]\",\"+\",\"++\",\"-\",\"--\",\"_\",\"<\",\">\",\"'\",\"''\",\"``\",'\"',\"!\",\"!=\",\"?\",\";\"]\n",
    "    wtbr = usual_stopwords + other_words + punctuation\n",
    "    \n",
    "    pattern01 = re.compile(r'[^a-z0-9]', flags=re.IGNORECASE)\n",
    "    pattern02 = re.compile(r'\\d+', flags=re.IGNORECASE)\n",
    "    pattern03 = re.compile(r'\\w$', flags=re.IGNORECASE)\n",
    "    \n",
    "    for plt in data['platform']:\n",
    "        count = 0\n",
    "        textlist = ['']\n",
    "        if data.loc[data['platform'] == plt, 'description'].values[0] != '' and data.loc[data['platform'] == plt, 'description'].values[0] != None:\n",
    "            if data.loc[data['platform'] == plt, 'description'].values[0] not in ['noinformationfound', 'errorreachingpage']:\n",
    "                textlist = textlist + re.sub(pattern01, ' ',data.loc[data['platform'] == plt, 'description'].values[0].lower()).split(' ')\n",
    "                count += 1\n",
    "        if data.loc[data['platform'] == plt, 'keywords'].values[0] != '' and data.loc[data['platform'] == plt, 'keywords'].values[0] != None:\n",
    "            if data.loc[data['platform'] == plt, 'keywords'].values[0] not in ['noinformationfound', 'errorreachingpage']:\n",
    "                textlist = textlist + re.sub(pattern01, ' ',data.loc[data['platform'] == plt, 'keywords'].values[0].lower()).split(' ')\n",
    "                count += 1\n",
    "        if data.loc[data['platform'] == plt, 'title'].values[0] != '' and data.loc[data['platform'] == plt, 'title'].values[0] != None:\n",
    "            if data.loc[data['platform'] == plt, 'title'].values[0] not in ['noinformationfound', 'errorreachingpage']:\n",
    "                textlist = textlist + re.sub(pattern01, ' ',data.loc[data['platform'] == plt, 'title'].values[0].lower()).split(' ')\n",
    "                count += 1\n",
    "        if data.loc[data['platform'] == plt, 'htext'].values[0] != '' and data.loc[data['platform'] == plt, 'htext'].values[0] != None:\n",
    "            if data.loc[data['platform'] == plt, 'htext'].values[0] not in ['noinformationfound', 'errorreachingpage']:\n",
    "                textlist = textlist + re.sub(pattern01, ' ',data.loc[data['platform'] == plt, 'htext'].values[0].lower()).split(' ')\n",
    "                count += 1\n",
    "                \n",
    "        for p in data.loc[data['platform'] == plt, 'params'].values[0].split(','):\n",
    "            allpwds = re.sub(pattern01, ' ', p.lower()).split(' ')\n",
    "            textlist = textlist + allpwds\n",
    "\n",
    "        #print(set(textlist))\n",
    "        \n",
    "        text = ''\n",
    "            \n",
    "        for e in set(textlist):\n",
    "            #assert type(e).__name__ == str, type(e)\n",
    "            if (e != '' or e != ' ') and not re.match(pattern02, e) and e not in wtbr:\n",
    "                #print(e)\n",
    "                if e in ['rants', 'rant']:\n",
    "                    e = 'blog'\n",
    "                text = text + ' ' + e\n",
    "                    \n",
    "        data.loc[data['platform'] == plt, 'alltext'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pdbackendclass['alltext'] = ''\n",
    "#pdbackendclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_primerclass['alltext'] = ''\n",
    "datapreparation(pd_primerclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_primerclass.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation 2 : TEST DATASET with resources of Gitter HelpFrontEnd chatroom (Jun-16 / Mar-17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING TEST FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pickle.load(open(directory+'test_treateddata_links.pkl','br'))\n",
    "len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test[list(test.keys())[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_test = pandas.DataFrame.from_dict(test)\n",
    "pd_test = pd_test.transpose()\n",
    "pd_test\n",
    "pd_test = pd_test.reset_index()\n",
    "pd_test = pd_test.rename(columns={'index':'platform'})\n",
    "pd_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACTING FROM TEST DATASET THE ALREADY CLASSIFIED PLATFORMS FOUND IN primer DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning that when running a classifier over only duplicated information found in the test, the classification was almost if not completely perfect (not shown). Extracting them from the test was required to avoid the effect of overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_primerclass['platform'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_primerclass[['platform','wiki']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_testwodup1 = pd_test.loc[~dp_test.platform.isin(pd_primerclass['platform'])]\n",
    "pd_testwodup1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paramsintostr(x):\n",
    "    #print(x)\n",
    "    if x == None:\n",
    "        return text\n",
    "    x = ','.join(list(x))\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_testwodup1['params'] = pd_testwodup1['params'].apply(paramsintostr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_testwodup1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getting_wikipedia(pd_testwodup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_testwodup1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datapreparation(pd_testwodup1)\n",
    "pd_testwodup1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST ROUND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting some classes after manual classification (affected by data formatting in spreadsheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_primerclass.loc[pd_primerclass.newclass.isin(['learn|tutorial|course|training|',\n",
    "       'learn|tutorial|course|training| tips|example',\n",
    "       'learn|tutorial|course|training|tips|example']), 'newclass'] = 'learn|tutorial|course|training| tips|example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_primerclass.loc[pd_primerclass.newclass.isin(['(text )?editor|interpreter|repl', '(text)?editor|interpreter|repl']), 'newclass'] = '(text )?editor|interpreter|repl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### primer DATA MODELLING - VECTOR MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect_r1 = CountVectorizer(ngram_range=(1,2))\n",
    "X_primercounts = count_vect_r1.fit_transform(pd_primerclass.alltext)\n",
    "X_primercounts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning MNB Classifier Class Reduction to 4 instead of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section was actually added eventually in an effort to verify how the data reduction would affect the ability of the MNB to improve classification. One aspect that I tried to tackle here was the **unbalanced classes**. Few interesting observations were taken but nothing that could use largerly to improve the results.\n",
    "\n",
    "The code is kept commented in case the reader thinks in trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd_primerclass['mnb_class'] = pd_primerclass['newclass']\n",
    "#pd_primerclass.loc[pd_primerclass.mnb_class.isin(['(text )?editor|interpreter|repl', '---',\n",
    "#       'cloud|platform|service', 'community|support|people|forum',\n",
    "#       'design|galler|template|theme',\n",
    "#       'manual|guide|docs',\n",
    "#       'on?(-|\\\\s)?demand|business|compan(y|ies)|enterprise',\n",
    "#       'searchtools', 'shop|commerce']), 'mnb_class'] = 'other'\n",
    "\n",
    "#pd_primerclass.loc[pd_primerclass.mnb_class == 'other', 'mnb_class'].describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYESIAN CLASSIFICATION 1\n",
    "\n",
    "Main Reference:\n",
    "* http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "* https://stats.stackexchange.com/questions/99667/naive-bayes-with-unbalanced-classes\n",
    "* http://www.cs.waikato.ac.nz/~eibe/pubs/FrankAndBouckaertPKDD06new.pdf\n",
    "* coindidentially !! -> http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py\n",
    "* http://www.programcreek.com/python/example/84841/sklearn.feature_extraction.text.CountVectorizer\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html (OJO: this is NOT the vectorizer, but a NORMALIZER!!)\n",
    "\n",
    "IMPORTANT:\n",
    "\n",
    "About the smoothing prior parameter $\\alpha$ in Multinomial Naive Bayesian Classsification:\n",
    "* https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplacian-smoothing-when-we-have-unknown-words-i\n",
    "\n",
    "Some interesting definitions?:\n",
    "* http://scikit-learn.org/stable/modules/multiclass.html\n",
    "* https://stackoverflow.com/questions/20461165/how-to-convert-pandas-index-in-a-dataframe-to-a-column\n",
    "\n",
    "A search query with interesting results:\n",
    "* \"sklearn.feature_extraction.text.CountVectorizer normalization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Counts for MNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized data was used for testing but it actually was counterproductive.\n",
    "\n",
    "A commented line is left for any normalization made if the reader is interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "#normalized_X_trainprimer_counts = sklearn.feature_extraction.text.TfidfTransformer(norm='l2').fit_transform(X_trainprimer_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_nb_r1 = MultinomialNB(alpha=0.1).fit(X_primercounts, pd_primerclass.new_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test1counts = count_vect_r1.transform(pd_testwodup1.alltext)\n",
    "#normalized_X_testround1_counts = sklearn.feature_extraction.text.TfidfTransformer(norm='l2').fit_transform(X_testround1_counts)\n",
    "predicted_nb_r1 = clf_nb_r1.predict(X_test1counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for platform, category in zip(pd_testwodup1.platform, predicted_nb_r1):\n",
    "    print('%r => %s' % (platform, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clf_nb_r1.predict_proba(X_testwodup1counts)\n",
    "#clf_nb_r1.get_params()\n",
    "#clf_nb_r1.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DECISION TREE CLASSIFICATION 1\n",
    "\n",
    "Main Reference:\n",
    "* http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "Some interesting definitions?:\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_dtm_r1 = tree.DecisionTreeClassifier()\n",
    "clf_dt_r1 = clf_dtm_r1.fit(X_primercounts, pd_primerclass.newclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test1counts = count_vect_r1.transform(pd_testwodup1.alltext)\n",
    "predicted_dt_r1 = clf_dt.predict(X_test1counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for platform, category in zip(pd_testwodup1.platform, predicted_dt_r1):\n",
    "    print('%r => %s' % (platform, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUND 2: adding some revised classifications of platforms to primer from test in Round 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Training Dataset Preparation - Adding Newly Classified Records with Extended Data and Concat to primer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if pathlib.Path(directory+'test_classes_rev_r1.csv').is_file():\n",
    "    pd_test1class = pandas.read_csv(open(directory+'test_classes_rev_r1.csv', 'r'))\n",
    "pd_test1class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_testwodup1class = pd_test1wodup.loc[pd_testwodup1.platform.isin(pd_test1class['platform'])]\n",
    "pd_testwodup1class.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_testwodup1class = pandas.merge(pd_testwodup1class, pd_testwodup1, on='platform')\n",
    "pd_testwodup1class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_combined1class = pandas.concat([pd_primerclass, pd_testwodup1class], ignore_index = True)\n",
    "pd_combined1class.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_combined1class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pdcombinedclass.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Test Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#taking the same name as above!\n",
    "pd_testwodup2 = pd_testwodup1.loc[~pd_testwodup1.platform.isin(pd_test2class['platform'])]\n",
    "pd_testwodup2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pd_testwodup2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data Model, with more data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect_r2 = CountVectorizer(ngram_range=(1,2))\n",
    "X_combined1counts = count_vect_r2.fit_transform(pd_combined1class.alltext)\n",
    "X_combined1counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting some classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_combined1class.loc[pd_combined1class.newclass.isin(['learn|tutorial|course|training|',\n",
    "       'learn|tutorial|course|training| tips|example',\n",
    "       'learn|tutorial|course|training|tips|example']), 'newclass'] = 'learn|tutorial|course|training| tips|example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_combined1class.loc[pd_combined1class.newclass.isin(['(text )?editor|interpreter|repl', '(text)?editor|interpreter|repl']), 'newclass'] = '(text )?editor|interpreter|repl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classification 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning MNB Classifier Class Reduction to 4 instead of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in round one: this part was for further evaluation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd_combined1class['mnb_class'] = pd_combined1class['newclass']\n",
    "#pd_combined1class.loc[pd_combined1class.mnb_class.isin(['(text )?editor|interpreter|repl', '---',\n",
    "#       'cloud|platform|service', 'community|support|people|forum',\n",
    "#       'design|galler|template|theme',\n",
    "#       'manual|guide|docs',\n",
    "#       'on?(-|\\\\s)?demand|business|compan(y|ies)|enterprise',\n",
    "#       'searchtools', 'shop|commerce']), 'mnb_class'] = 'other'0\n",
    "#\n",
    "#pd_combined1class.loc[pd_combined1class.mnb_class == 'other', 'mnb_class'].describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Counts for MNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in round one: this part was for further evaluation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "#normalized_X_train_counts = sklearn.feature_extraction.text.TfidfTransformer(norm='l2').fit_transform(X_combined1counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_nb_r2 = MultinomialNB(alpha=0.1).fit(X_combined1counts, pd_combined1class.newclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_testwodup2counts = count_vect.transform(pd_testwodup2.alltext)\n",
    "#normalized_X_nonew_counts = sklearn.feature_extraction.text.TfidfTransformer(norm='l2').fit_transform(X_testwodup2counts)\n",
    "predicted_nb_r2 = clf_nb_r2.predict( X_testwodup2counts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for platform, category in zip(pd_testwodup2.platform, predicted_nb_r2):\n",
    "    print('%r => %s' % (platform, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clf_nb_r2.predict_proba(X_testwodup2counts)\n",
    "#clf_nb_r2.get_params()\n",
    "#clf_nb_r2.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_dtm_r2 = tree.DecisionTreeClassifier()\n",
    "clf_dt_r2 = clf_dtm_r2.fit(X_combined1counts, pd_combined1class.newclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_testwodup2counts = count_vect.transform(pd_testwodup2.alltext)\n",
    "predicted_dt_r2 = clf_dt.predict(X_testwodup2counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for platform, category in zip(pd_testwodup2.platform, predicted_dt_r2):\n",
    "    print('%r => %s' % (platform, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ROUND 3: Last round, again increasing training dataset with revised classifications at Round 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Training Dataset Preparation - Adding Newly Classified Records with Extended Data and Concat to primer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if pathlib.Path(directory+'test_classes_rev_r2.csv').is_file():\n",
    "    pd_test2class = pandas.read_csv(open(directory+'test_classes_rev_r2.csv', 'r'))\n",
    "pd_test2class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_testwodup2class = pd_testwodup1.loc[pd_testwodup1.platform.isin(pd_test2class['platform'])]\n",
    "pd_testwodup2class.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_testwodup2class = pandas.merge(pd_testwodup2class, pd_testwodup1, on='platform')\n",
    "pd_testwodup2class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_combined2class = pandas.concat([pd_combined1class, pd_testwodup2class], ignore_index = True)\n",
    "pd_combined2class.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_combined2class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pdcombinedclass.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Test Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#taking the same name as above!\n",
    "pd_testwodup3 = pd_testwodup2.loc[~pd_testwodup2.platform.isin(pd_test2class['platform'])]\n",
    "pd_testwodup3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pd_testwodup3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data Model, with more data 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect_r3 = CountVectorizer(ngram_range=(1,2))\n",
    "X_combined2counts = count_vect_r2.fit_transform(pd_combined2class.alltext)\n",
    "X_combined2counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting some classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_combined2class.loc[pd_combined2class.newclass.isin(['learn|tutorial|course|training|',\n",
    "       'learn|tutorial|course|training| tips|example',\n",
    "       'learn|tutorial|course|training|tips|example']), 'newclass'] = 'learn|tutorial|course|training| tips|example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_combined2class.loc[pd_combined2class.newclass.isin(['(text )?editor|interpreter|repl', '(text)?editor|interpreter|repl']), 'newclass'] = '(text )?editor|interpreter|repl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classification 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning MNB Classifier Class Reduction to 4 instead of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in round one: this part was for further evaluation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd_combined2class['mnb_class'] = pd_combined2class['newclass']\n",
    "#pd_combined2class.loc[pd_combined2class.mnb_class.isin(['(text )?editor|interpreter|repl', '---',\n",
    "#       'cloud|platform|service', 'community|support|people|forum',\n",
    "#       'design|galler|template|theme',\n",
    "#       'manual|guide|docs',\n",
    "#       'on?(-|\\\\s)?demand|business|compan(y|ies)|enterprise',\n",
    "#       'searchtools', 'shop|commerce']), 'mnb_class'] = 'other'\n",
    "#\n",
    "#pd_combined2class.loc[pd_combined1class.mnb_class == 'other', 'mnb_class'].describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Counts for MNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in round one: this part was for further evaluation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "#normalized_X_train_counts = sklearn.feature_extraction.text.TfidfTransformer(norm='l2').fit_transform(X_combined2counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_nb_r3 = MultinomialNB(alpha=0.1).fit(X_combined2counts, pd_combined2class.newclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_testwodup3counts = count_vect.transform(pd_testwodup3.alltext)\n",
    "#normalized_X_nonew_counts = sklearn.feature_extraction.text.TfidfTransformer(norm='l2').fit_transform(X_testwodup2counts)\n",
    "predicted_nb_r3 = clf_nb_r3.predict( X_testwodup3counts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for platform, category in zip(pd_testwodup3.platform, predicted_nb_r3):\n",
    "    print('%r => %s' % (platform, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clf_nb_r3.predict_proba(X_testwodup3counts)\n",
    "#clf_nb_r3.get_params()\n",
    "#clf_nb_r3.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_dtm_r3 = tree.DecisionTreeClassifier()\n",
    "clf_dt_r3 = clf_dtm_r3.fit(X_combined2counts, pd_combined2class.newclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_testwodup3counts = count_vect.transform(pd_testwodup3.alltext)\n",
    "predicted_dt_r3 = clf_dt.predict(X_testwodup3counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for platform, category in zip(pd_testwodup3.platform, predicted_dt_r3):\n",
    "    print('%r => %s' % (platform, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING THE CLASSIFIED PLATFORMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
